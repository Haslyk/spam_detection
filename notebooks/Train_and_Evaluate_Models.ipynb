{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. SETUP AND ENVIRONMENT\n",
    "# ==============================================================================\n",
    "# This script trains and evaluates the best-performing model, DistilBERT + Features,\n",
    "# Email Classification Using Deep and Structural Features\".\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = \"../data/balanced_dataset.csv\"\n",
    "PRETRAINED_DISTILBERT_PATH = \"distilbert-base-uncased\" # Using public model for reproducibility\n",
    "MODEL_SAVE_DIR = \"../saved_models/\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "from transformers import __version__ as transformers_version\n",
    "print(f\"Transformers Version: {transformers_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. DATA LOADING AND PREPROCESSING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Loading and Preprocessing Data ---\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.dropna(subset=['text', 'label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "print(f\"Dataset loaded successfully. Total rows: {len(df)}\")\n",
    "print(\"Label distribution:\\n\", df['label'].value_counts(normalize=True))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text_for_transformer(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, flags=re.I|re.A)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "tqdm.pandas(desc=\"Cleaning Text\")\n",
    "print(\"\\nApplying text cleaning for Transformer model...\")\n",
    "df['cleaned_text'] = df['text'].progress_apply(clean_text_for_transformer)\n",
    "print(\"Text cleaning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Engineering Features ---\")\n",
    "print(\"Extracting structural features from raw text...\")\n",
    "\n",
    "df['text_len'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "def uppercase_ratio(text):\n",
    "    if not isinstance(text, str) or len(text) == 0: return 0.0\n",
    "    return sum(1 for char in text if char.isupper()) / len(text)\n",
    "df['uppercase_ratio'] = df['text'].apply(uppercase_ratio)\n",
    "\n",
    "def punctuation_count(text):\n",
    "    if not isinstance(text, str): return 0\n",
    "    return len(re.findall(r'[!\\\"#$%&\\'()*+,-./:;<=>?@\\[\\]^_`{|}~]', text))\n",
    "df['punctuation_count'] = df['text'].apply(punctuation_count)\n",
    "\n",
    "numerical_feature_cols = ['text_len', 'word_count', 'uppercase_ratio', 'punctuation_count']\n",
    "numerical_features = df[numerical_feature_cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_features_scaled = scaler.fit_transform(numerical_features)\n",
    "with open(os.path.join(MODEL_SAVE_DIR, 'scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Structural features extracted and scaled. Scaler saved.\")\n",
    "\n",
    "print(\"\\nGenerating contextual embeddings with DistilBERT...\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(PRETRAINED_DISTILBERT_PATH)\n",
    "distilbert_base = TFDistilBertModel.from_pretrained(PRETRAINED_DISTILBERT_PATH)\n",
    "\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE_EMBEDDING = 32\n",
    "\n",
    "all_embeddings = []\n",
    "texts = df['cleaned_text'].tolist()\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE_EMBEDDING), desc=\"Generating Embeddings\"):\n",
    "    batch = texts[i:i+BATCH_SIZE_EMBEDDING]\n",
    "    inputs = tokenizer(batch, return_tensors='tf', truncation=True, padding='max_length', max_length=MAX_LEN)\n",
    "    outputs = distilbert_base(inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    all_embeddings.append(cls_embeddings)\n",
    "text_embeddings = np.vstack(all_embeddings)\n",
    "print(f\"Embeddings generated. Shape: {text_embeddings.shape}\")\n",
    "\n",
    "print(\"\\nCombining features and splitting data...\")\n",
    "labels = df['label'].values\n",
    "combined_features = np.concatenate([text_embeddings, numerical_features_scaled], axis=1)\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    combined_features, labels, test_size=0.20, random_state=RANDOM_STATE, stratify=labels\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.10, random_state=RANDOM_STATE, stratify=y_train_full\n",
    ")\n",
    "print(f\"Data split complete: Train={X_train.shape}, Validation={X_val.shape}, Test={X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. MODEL TRAINING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Model Training ---\")\n",
    "def build_classifier(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "classifier = build_classifier((X_train.shape[1],))\n",
    "classifier.summary()\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "proc = psutil.Process(os.getpid())\n",
    "mem_before = proc.memory_info().rss / (1024 ** 2)\n",
    "start_time = time.time()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = classifier.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "end_time = time.time()\n",
    "mem_after = proc.memory_info().rss / (1024 ** 2)\n",
    "\n",
    "training_time_seconds = end_time - start_time\n",
    "peak_cpu_mem_mb = max(mem_before, mem_after)\n",
    "print(f\"\\nTraining complete in {training_time_seconds:.2f} seconds.\")\n",
    "print(f\"Peak CPU Memory Usage: {peak_cpu_mem_mb:.2f} MB\")\n",
    "\n",
    "classifier.save(os.path.join(MODEL_SAVE_DIR, 'distilbert_features_classifier.h5'))\n",
    "print(\"Classifier model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. EVALUATION\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Final Evaluation on Test Set ---\")\n",
    "loss, accuracy = classifier.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"  Test Loss: {loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "y_pred_probs = classifier.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Ham (0)', 'Spam (1)']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining History:\")\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spamdetector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
